{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Evan\\AppData\\Local\\conda\\conda\\envs\\env_futures\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim,os,sklearn,tqdm\n",
    "\n",
    "# from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR_OF_PROJECT=\"C:\\\\Users\\\\Evan\\\\MyFile\\\\Fortune-street\\\\007 oil_price\\\\oilprice_news_analyze\\\\\"\n",
    "\n",
    "#ROOT=\"D:/work/fortune_street/002 news_analyze/002 data/002 corpus_data/\"\n",
    "\n",
    "root_dir=ROOT_DIR_OF_PROJECT+\"002 data/002 corpus_data/\"\n",
    "list_of_source=os.listdir(root_dir+\"raw_csv_data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "aSource=list_of_source[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f289ec8c4ab4caa98f0627db1614914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=73), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "041dddc4df4b4f5a858c8f8fa4270436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=278), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25f10ae2643843b7b886888780d7d535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=373), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n",
      "tokenize fail\n",
      "nan\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f61a854e3a34a0290efd01eb646a1a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for aSource in list_of_source:\n",
    "    preprocessor=Preprocessor()\n",
    "    list_of_filenames=os.listdir(root_dir+\"raw_csv_data/\"+aSource) \n",
    "    for aFile in tqdm.tqdm_notebook(list_of_filenames):\n",
    "        filedir=root_dir+\"raw_csv_data/\"+aSource+\"/\"+aFile\n",
    "        try:\n",
    "            news_data_df=pd.read_csv(root_dir+\"raw_csv_data/\"+aSource+\"/\"+aFile)\n",
    "            news_contents=news_data_df['content']\n",
    "            news_titles=news_data_df['title']\n",
    "            news_datetime=news_data_df['publish_datetime']\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "        #preprocess with no stem \n",
    "        news_tokens=preprocessor.get_tokens_in_sent(news_contents)\n",
    "\n",
    "\n",
    "    #     # save the ones with no stemming\n",
    "        savedir=root_dir+\"token_in_sent_txt_data/\"+aSource+\"/\"+news_datetime[0]+\"/\"\n",
    "        if not os.path.exists(savedir):\n",
    "            os.makedirs(savedir)\n",
    "        for a_news_id,a_news in enumerate(zip(news_titles,news_tokens)):\n",
    "            a_title,a_tokens=a_news\n",
    "            with open(savedir+\"news_\"+str(a_news_id)+\".txt\", \"w\",encoding=\"utf8\") as text_file:\n",
    "                text_file.writelines(a_title.lower()+\"\\n\")\n",
    "                for token in a_tokens:\n",
    "                    text_file.writelines(\" \".join(token)+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # dictionary = counter\n",
    "    # datetimestr=datetime.today().date().strftime(\"%Y_%m_%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self,stem=False):\n",
    "        self.stem=stem\n",
    "        \n",
    "    def stem_and_other_stuff(self,each_news,do_tokenize=False):\n",
    "        if do_tokenize:\n",
    "            each_news=word_tokenize(each_news)\n",
    "        ps=PorterStemmer()\n",
    "        return([ps.stem(word.lower()) for word in each_news if word.isalpha()])\n",
    "        \n",
    "    def check_alpha_tolower(self,each_news,do_tokenize=False):\n",
    "        if do_tokenize:\n",
    "            each_news=word_tokenize(each_news)\n",
    "        return([word.lower() for word in each_news if word.isalpha()])\n",
    "    \n",
    "    def get_tokenized_sents(self,sents):\n",
    "        try:\n",
    "            return([self.check_alpha_tolower(sent,do_tokenize=True) for sent in sents])\n",
    "        except:\n",
    "            print(\"tokenize sents fail\")\n",
    "            print(content)\n",
    "            return([])\n",
    "    \n",
    "    def my_word_tokenize(self,content):\n",
    "        try:\n",
    "            return(word_tokenize(content))\n",
    "        except:\n",
    "            if content==np.nan:\n",
    "                print(\"tokenize fail nan\")\n",
    "            else:\n",
    "                print(\"tokenize fail\")\n",
    "                print(content)\n",
    "            return([])\n",
    "        \n",
    "    def my_sent_tokenize(self,content):\n",
    "        try:\n",
    "            return(sent_tokenize(content))\n",
    "        except:\n",
    "            if content==np.nan:\n",
    "                print(\"tokenize fail nan\")\n",
    "            else:\n",
    "                print(\"tokenize fail\")\n",
    "                print(content)\n",
    "            return([])\n",
    "        \n",
    "        \n",
    "    def get_tokens(self,content):\n",
    "        token_content=content.apply(self.my_word_tokenize)\n",
    "        if self.stem:        \n",
    "            tokens=token_content.apply(self.stem_and_other_stuff)\n",
    "        else:\n",
    "            tokens=token_content.apply(self.check_alpha_tolower)\n",
    "        return(tokens)\n",
    "    \n",
    "    def get_tokens_in_sent(self,content):\n",
    "        sent_content=content.apply(self.my_sent_tokenize)\n",
    "        tokens=[self.get_tokenized_sents(sents) for sents in sent_content]\n",
    "        return(tokens)\n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    def get_counter(self,tokens):\n",
    "        content_counter = Counter()\n",
    "        for news in tokens:\n",
    "            content_counter.update(news)\n",
    "        return(content_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "    def __iter__(self):\n",
    "        sources=os.listdir(self.dirname)\n",
    "        for aSource in sources:\n",
    "            dates=os.listdir(os.path.join(self.dirname,aSource))\n",
    "            for aDate in dates:\n",
    "                filepath_to_date=os.path.join(self.dirname,aSource,aDate)\n",
    "                for fname in os.listdir(filepath_to_date):\n",
    "                    for line in open(os.path.join(self.dirname,aSource,aDate,fname),encoding=\"utf8\"):\n",
    "                        yield line.split()\n",
    "                \n",
    "                \n",
    "sentences = MySentences('C:\\\\Users\\\\Evan\\\\MyFile\\\\Fortune-street\\\\007 oil_price\\\\oilprice_news_analyze\\\\002 data\\\\002 co\\\n",
    "rpus_data\\\\token_in_sent_txt_data') # a memory-friendly iterator\n",
    "model = gensim.models.Word2Vec(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"tmp\")\n",
    "model.save(\"tmp/w2v_model_20190321\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
