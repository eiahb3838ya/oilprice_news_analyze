{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk,re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from math import *\n",
    "from datetime import datetime\n",
    "from progressbar import progressbar\n",
    "import matplotlib.pyplot as plt\n",
    "# import \n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_vector(this_year_token,pairwise_with_windows_list,mystopword,window_size):\n",
    "    this_year_vs=[]\n",
    "    for tokenized_article in progressbar(this_year_token,prefix=\"word to vector\"):\n",
    "        finder = nltk.BigramCollocationFinder.from_words([word for word in tokenized_article if word not in mystopword.stop_word],window_size=window_size)\n",
    "        this_vs= {key: 0 for key in pairwise_with_windows_list}\n",
    "        for pair,times in finder.ngram_fd.items():\n",
    "            if pair in this_vs.keys():\n",
    "                this_vs[pair]=times\n",
    "        this_year_vs.append(this_vs)            \n",
    "    return(this_year_vs)\n",
    "\n",
    "def train(train_documents,vector_size=300):\n",
    "    model = Doc2Vec(train_documents, vector_size=vector_size, window=4, min_count=2, workers=12)\n",
    "    model.train(train_documents,total_examples=model.corpus_count,epochs=30)\n",
    "    return(model)\n",
    "def get_content(data_df):\n",
    "    content=data_df.content\n",
    "    content.index = pd.DatetimeIndex(content.index)\n",
    "    content=content.dropna(how=\"any\")\n",
    "    return(content)\n",
    "class Preprocessor:\n",
    "    def __init__(self,stopword=[],use_stem=False,use_summarize=True,summarize_word_count=200):\n",
    "        self.use_stem=use_stem\n",
    "        self.use_summarize=use_summarize\n",
    "        self.summarize_word_count=summarize_word_count\n",
    "        self.stopword=stopword\n",
    "    def stem_and_other_stuff(self,each_news):\n",
    "        ps=PorterStemmer()\n",
    "        return([ps.stem(word.lower()) for word in each_news.split(\" \") if word.isalpha() and word not in self.stopword])\n",
    "    \n",
    "    def check_alpha_tolower(self,each_news):\n",
    "        return([word.lower() for word in each_news.split(\" \") if word.isalpha()])\n",
    "        \n",
    "    def get_tokenized_content(self,content):\n",
    "        tokenized_content_s=content.apply(word_tokenize)\n",
    "        if self.use_stem:        \n",
    "            output_token=tokenized_content_s.apply(self.stem_and_other_stuff)\n",
    "        else:\n",
    "            output_token=tokenized_content_s.apply(self.check_alpha_tolower)\n",
    "        return(output_token)\n",
    "    \n",
    "    def get_counter(self,content):\n",
    "        tokenized_content_s=self.get_tokenized_content(content)\n",
    "        content_counter=Counter()\n",
    "        for aStemmed_token in tokenized_content_s:\n",
    "            content_counter.update(aStemmed_token)\n",
    "#             self.counter = content_counter\n",
    "        return(content_counter)\n",
    "    \n",
    "    def get_summarize(self,content,summarize_ratio=None):\n",
    "        if summarize_ratio:\n",
    "            return(content.apply(lambda txt:summarize(txt,word_count = summarize_word_count)))\n",
    "        else:\n",
    "            return(content.apply(lambda txt:summarize(txt,word_count = self.summarize_word_count)))\n",
    "#             return(content.apply(lambda txt:summarize(txt,ratio = self.summarize_ratio)))\n",
    "    def preprocess(self,content):\n",
    "        if self.use_summarize:\n",
    "            content=content.loc[content.apply(clean_text_by_sentences).apply(list).apply(len).apply(lambda x:x>1)]\n",
    "            content=self.get_summarize(content)\n",
    "        content_counter=self.get_counter(content)\n",
    "        return(content_counter)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "class MyStopWord:\n",
    "    def __init__(self,content_counter,most_common=100,stop_word=None):\n",
    "        from nltk.corpus import stopwords\n",
    "        self.counter_stop_word=[word for word,time in content_counter.most_common(most_common)]\n",
    "        self.user_keep=[]\n",
    "        self.user_define=[]\n",
    "        if stop_word:\n",
    "            self.stop_word=stop_word\n",
    "        else:\n",
    "            self.stop_word=set(self.counter_stop_word+stopwords.words('english')) \n",
    "    def keep(self,word):\n",
    "        self.user_keep.append(word)\n",
    "        self.stop_word.discard(word)\n",
    "    def define(self,word):\n",
    "        self.user_define.append(word)\n",
    "        self.stop_word.add(word)\n",
    "\n",
    "class Unigram:\n",
    "    def __init__(self,target_counter,other_counter):\n",
    "        self.target_counter = target_counter\n",
    "        self.other_counter = other_counter\n",
    "        \n",
    "    def get_different_corpus_set(self,mystopword,TF_OTHER_THRESHOLD=20,TF_TARGET_THRESHOLD=5):\n",
    "        other_corpus_set=set(key for key,times in self.other_counter.items() if times>TF_OTHER_THRESHOLD)-mystopword.stop_word\n",
    "        target_corpus_set=set(key for key,times in self.target_counter.items() if times>TF_TARGET_THRESHOLD)-mystopword.stop_word\n",
    "        self.different_corpus_set = target_corpus_set-other_corpus_set\n",
    "\n",
    "class Bigram:\n",
    "    def __init__(self,token):\n",
    "        self.token = token\n",
    "    def count_word_pair_with_windows(self,window_size,mystopword):\n",
    "        stop_word = mystopword.stop_word\n",
    "        self.pair_counts = Counter()\n",
    "        self.pair_distance_counts = Counter()\n",
    "        for tokens in self.token:\n",
    "            for i in range(len(tokens) - 1):\n",
    "                for distance in range(1, window_size):\n",
    "                    if i + distance < len(tokens):\n",
    "                        w1 = tokens[i]\n",
    "                        w2 = tokens[i + distance]\n",
    "                        if w1 not in stop_word and w2 not in stop_word:\n",
    "                            self.pair_distance_counts[(w1, w2, distance)] += 1\n",
    "                            self.pair_counts[(w1, w2)] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-12 2018-04 2018-05 2018-06\n"
     ]
    }
   ],
   "source": [
    "# for time in a:\n",
    "#     TRAIN_START_DATE=pd.to_datetime(time).strftime(\"%Y-%m\")\n",
    "#     TRAIN_INTIVAL=11\n",
    "#     TEST_INTIVAL=1\n",
    "\n",
    "#     from_date = TRAIN_START_DATE\n",
    "#     to_date = str(np.datetime64(TRAIN_START_DATE)+np.timedelta64(TRAIN_INTIVAL,'M'))\n",
    "#     test_from_date=to_date\n",
    "#     test_to_date=str(np.datetime64(to_date)+np.timedelta64(TEST_INTIVAL,'M'))\n",
    "#     print(\"from_date\",from_date,\"to_date\",to_date,\"test_from_date\",test_from_date,\"test_to_date\",test_to_date)\n",
    "\n",
    "TRAIN_START_DATE = \"2017-12\"\n",
    "\n",
    "TRAIN_INTIVAL = 4\n",
    "TEST_INTIVAL = 1\n",
    "\n",
    "from_date = TRAIN_START_DATE\n",
    "to_date = str(np.datetime64(TRAIN_START_DATE) +\n",
    "              np.timedelta64(TRAIN_INTIVAL, 'M'))\n",
    "test_from_date = str(np.datetime64(to_date) +\n",
    "              np.timedelta64(1, 'M'))\n",
    "test_to_date = str(np.datetime64(test_from_date)+np.timedelta64(TEST_INTIVAL, 'M'))\n",
    "\n",
    "print(from_date,to_date,test_from_date,test_to_date)\n",
    "window_size=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv(\"../../data/crawler_news_data/oilprice_news.csv\")\n",
    "raw_df_cnbc = pd.read_csv(\"../../data/crawler_news_data/cnbc_oil_news.csv\")\n",
    "data_df=raw_df.sort_values(by=\"publish_datetime\",ascending=True).set_index('publish_datetime')\n",
    "data_df_cnbc = raw_df_cnbc.sort_values(by=\"story_publish_datetime\",ascending=True).set_index('story_publish_datetime')\n",
    "data_df_oilprice = pd.DataFrame({\"date\":raw_df.publish_datetime,\"content\":raw_df.content})\n",
    "data_df_cnbc = pd.DataFrame({\"date\":raw_df_cnbc.story_publish_datetime,\"content\":raw_df_cnbc.story_full_article})\n",
    "data_df_oilprice_cnbc = data_df_oilprice.append(data_df_cnbc)\n",
    "data_df_oilprice_cnbc = data_df_oilprice_cnbc.sort_values(by=\"date\",ascending=True).set_index('date')\n",
    "raw_content = get_content(data_df_oilprice_cnbc)\n",
    "train_content = raw_content[from_date:to_date]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "to counter100% (635 of 635) |############| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "all_year_preprocessor = Preprocessor(content=train_content)\n",
    "all_year_preprocessor.to_counter()\n",
    "mystopword=MyStopWord(content_counter=all_year_preprocessor.counter,most_common=100)\n",
    "mystopword.define('c')\n",
    "mystopword.keep('demand')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用Target corpus - Other corpus find dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. effectivate news date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "effective_news_df = pd.read_csv(\"../../data/crude_oil_price/effective_news_date_percentage_positive.csv\")\n",
    "effective_news_date = effective_news_df['date']\n",
    "effective_news_date=pd.DatetimeIndex(effective_news_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. find target and other corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "to counter100% (53 of 53) |##############| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "to counter100% (582 of 582) |############| Elapsed Time: 0:00:00 Time:  0:00:00\n"
     ]
    }
   ],
   "source": [
    "train_content = raw_content[from_date:to_date]\n",
    "target_content = train_content.loc[train_content.index.isin(effective_news_date.values)]\n",
    "other_content = train_content.loc[~train_content.index.isin(effective_news_date.values)]\n",
    "target_preprocessor = Preprocessor(content = target_content)\n",
    "other_preprocessor = Preprocessor(content= other_content)\n",
    "target_preprocessor.to_counter()\n",
    "other_preprocessor.to_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. find bigram dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(pairwise_dictionary) 852\n"
     ]
    }
   ],
   "source": [
    "TF_TARGET_THRESHOLD=1\n",
    "OTHER_TARGET_THRESHOLD=5\n",
    "\n",
    "target_bigram = Bigram(token=target_preprocessor.tokens)\n",
    "other_bigram = Bigram(token=other_preprocessor.tokens)\n",
    "\n",
    "target_bigram.count_word_pair_with_windows(mystopword=mystopword,window_size=window_size)\n",
    "other_bigram.count_word_pair_with_windows(mystopword=mystopword,window_size=window_size)\n",
    "\n",
    "target_corpus_set=set([key for key,times in target_bigram.pair_counts.most_common(1000) if times>TF_TARGET_THRESHOLD])\n",
    "other_corpus_set=set([key for key,times in other_bigram.pair_counts.items() if times>OTHER_TARGET_THRESHOLD])\n",
    "\n",
    "pairwise_dictionary = target_corpus_set - other_corpus_set\n",
    "print(\"len(pairwise_dictionary)\",len(pairwise_dictionary))\n",
    "# pairwise_dictionary=[pair for pair,count in pairwise_dictionary]\n",
    "# pairwise_dictionary = set(target_bigram.pair_counts.most_common(100)) - set(other_bigram.pair_counts)\n",
    "# pairwise_dictionary=[pair for pair,count in pairwise_dictionary]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. word to vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "to counter100% (635 of 635) |############| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "word to vector100% (635 of 635) |########| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "train_preprocessor = Preprocessor(content=train_content)\n",
    "train_preprocessor.to_counter()\n",
    "train_vs = word_to_vector(train_preprocessor.tokens,pairwise_dictionary,mystopword,window_size)\n",
    "train_vs_df=pd.DataFrame(train_vs)\n",
    "train_vs_df=train_vs_df.set_index(pd.DatetimeIndex(train_content.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: 43.60377358490566\n",
      "other: 0.9518900343642611\n"
     ]
    }
   ],
   "source": [
    "print(\"target:\",train_vs_df.loc[train_content.index.isin(effective_news_date.values)].sum(axis=1).mean())\n",
    "print(\"other:\",train_vs_df.loc[~train_content.index.isin(effective_news_date.values)].sum(axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vs_df['tags'] = 0\n",
    "train_vs_df.loc[train_vs_df.index.isin(effective_news_date.values),'tags']=1\n",
    "train_vs_df.to_csv(\"../../data/train_test_dataset/oilprice_cnbc_new_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(test_to_date):\n",
    "    test_content = raw_content[test_from_date:test_to_date]\n",
    "else:\n",
    "    test_content = raw_content[test_from_date:]\n",
    "# effective_date = pd.read_csv(\"../../data/crude_oil_price/effective_news_date_percentage_positive.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "date\n",
       "2018-05-01                                                     \n",
       "2018-05-01    Apple's second-quarter earnings beat on Tuesda...\n",
       "2018-05-01    The increasingly volatile stock market has for...\n",
       "2018-05-01    Iran's fragile economic recovery is in jeopard...\n",
       "2018-05-01    Oil prices popped on Wednesday as the U.S. dol...\n",
       "2018-05-01    Oil prices look to have climbed to unsustainab...\n",
       "2018-05-01    BP beat analyst expectations on Tuesday, as hi...\n",
       "2018-05-01    BP's finance chief does not believe there is a...\n",
       "2018-05-02    The U.S. dollar has gained strength in recent ...\n",
       "2018-05-02    China has become the world’s largest oil impor...\n",
       "2018-05-02    “$60 is like the new $100,” Dallas Fed economi...\n",
       "2018-05-02    Oil-importing countries in the Middle East and...\n",
       "2018-05-02    Higher oil prices won't change the pace of Sau...\n",
       "2018-05-02    Oil prices rose on Thursday, boosted by OPEC p...\n",
       "2018-05-02    After yesterday the American Petroleum Institu...\n",
       "2018-05-03    China’s Sinopec will cut its June imports of c...\n",
       "2018-05-03    OPEC could achieve its goal of eliminating the...\n",
       "2018-05-03    Since the dramatic and unprecedented advances ...\n",
       "2018-05-03    Buckle up, America. Gas prices have a hit a le...\n",
       "2018-05-03    Oil prices strengthened on Friday as global su...\n",
       "2018-05-04    In recent weeks I’ve commented on the powerful...\n",
       "2018-05-04    The Iran nuclear deal is on the brink of colla...\n",
       "2018-05-06    Oil prices advanced last week amid concerns th...\n",
       "2018-05-06    Oil prices gave up their gains on Monday after...\n",
       "2018-05-07    As the world awaits U.S. President Donald Trum...\n",
       "2018-05-07    President Donald Trump announced in a tweet on...\n",
       "2018-05-07    When CNBC's Jim Cramer thinks the stock market...\n",
       "2018-05-07    Oil prices retreated from 3-1/2 year highs on ...\n",
       "2018-05-07    President Donald Trump says he will announce h...\n",
       "2018-05-07    President Donald Trump appears poised to withd...\n",
       "                                    ...                        \n",
       "2018-06-28    U.S. oil prices rose to a three-and-a-half yea...\n",
       "2018-06-28    But disruptions, record demand leave little sp...\n",
       "2018-06-28    SINGAPORE, June 29 (Reuters) - U.S. oil prices...\n",
       "2018-06-28    U.S. CRUDE FUTURES RISE MORE THAN $1 TO $73.83...\n",
       "2018-06-28    LONDON, June 28 (Reuters) - Oil prices steadie...\n",
       "2018-06-28    President Donald Trump’s sustained bid to disr...\n",
       "2018-06-28    Energy Secretary Rick Perry on Thursday confir...\n",
       "2018-06-28    LONDON, June 28 (Reuters) - Oil prices steadie...\n",
       "2018-06-28    NEW YORK, June 28 (Reuters) - Oil prices climb...\n",
       "2018-06-28    (Adds strategist quotes and details on activit...\n",
       "2018-06-28    NEW YORK, June 28 (Reuters) - U.S. oil prices ...\n",
       "2018-06-28    SINGAPORE, June 29 (Reuters) - Oil prices fell...\n",
       "2018-06-28    Ethiopia began on Thursday its first-ever crud...\n",
       "2018-06-28    Oil supply outages are multiplying around the ...\n",
       "2018-06-28    SINGAPORE, June 29 (Reuters) - Oil prices dipp...\n",
       "2018-06-28    NEW YORK, June 28 (Reuters) - U.S. oil prices ...\n",
       "2018-06-29    tight@ SINGAPORE, June 29 (Reuters) - Oil pric...\n",
       "2018-06-29     Oil prices rose on Friday as U.S. sanctions a...\n",
       "2018-06-29    U.S. CRUDE EXTENDS GAINS, UP BY MORE THAN $1 T...\n",
       "2018-06-29    market remains tight@ (Corrects spelling of Su...\n",
       "2018-06-29    tight@ But oil markets remain tight on outages...\n",
       "2018-06-29    June 29 (Reuters) - Oil prices look set to sta...\n",
       "2018-06-29    U.S. CRUDE FUTURES RISE TO SESSION HIGH OF $74...\n",
       "2018-06-29    With oil rallying 14 percent in the second qua...\n",
       "2018-06-29     WASHINGTON — The retirement of Justice Anthon...\n",
       "2018-06-29    Two and a half years after the U.S. removed ex...\n",
       "2018-06-29    Most analyses of the 2014 oil price crash and ...\n",
       "2018-06-29    According to Bank of America Merrill Lynch, oi...\n",
       "2018-06-30     President Donald Trump said in a tweet on Sat...\n",
       "2018-06-30                                                     \n",
       "Name: content, Length: 664, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "to counter100% (664 of 664) |############| Elapsed Time: 0:00:00 Time:  0:00:00\n",
      "word to vector100% (664 of 664) |########| Elapsed Time: 0:00:01 Time:  0:00:01\n"
     ]
    }
   ],
   "source": [
    "test_preprocessor = Preprocessor(content=test_content)\n",
    "test_preprocessor.to_counter()\n",
    "test_vs = word_to_vector(test_preprocessor.tokens,pairwise_dictionary,mystopword,window_size)\n",
    "test_vs_df=pd.DataFrame(test_vs)\n",
    "test_vs_df=test_vs_df.set_index(pd.DatetimeIndex(test_content.index))\n",
    "test_vs_df['tags'] = 0\n",
    "###############挫賽了拉\n",
    "test_vs_df.loc[test_vs_df.index.isin(effective_news_date.values),'tags'] =1\n",
    "###############挫賽了拉\n",
    "test_vs_df.to_csv(\"../../data/train_test_dataset/oilprice_cnbc_new_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target: 1.9150943396226414\n",
      "other: 0.8297491039426523\n"
     ]
    }
   ],
   "source": [
    "print(\"target:\",test_vs_df.loc[test_content.index.isin(effective_news_date.values)].sum(axis=1).mean())\n",
    "print(\"other:\",test_vs_df.loc[~test_content.index.isin(effective_news_date.values)].sum(axis=1).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## do some experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vs_sum=test_vs_df.loc[~test_content.index.isin(effective_news_date.values)].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preprocessor.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
