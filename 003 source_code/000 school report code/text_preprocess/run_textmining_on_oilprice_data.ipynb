{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk,re\n",
    "import pandas as pd\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "from math import *\n",
    "from datetime import datetime\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self,path=\"\",content=False):\n",
    "        if len(path)>1:\n",
    "            raw_df = pd.read_csv(path)\n",
    "            self.data_df = raw_df.sort_values(by=\"publish_datetime\",ascending=True).set_index('publish_datetime')\n",
    "            content=self.data_df.content\n",
    "            content.index = pd.DatetimeIndex(content.index)\n",
    "            content=content.dropna(how=\"any\")\n",
    "            self.content = content\n",
    "        else:\n",
    "            self.content = content\n",
    "    def stem_and_other_stuff(self,each_news):\n",
    "        ps=PorterStemmer()\n",
    "        return([ps.stem(word.lower()) for word in each_news if word.isalpha()])\n",
    "    def check_alpha_tolower(self,each_news):\n",
    "        return([word.lower() for word in each_news if word.isalpha()])\n",
    "    def get_content_from_date(self,from_date,to_date):\n",
    "        self.content = self.content[from_date:to_date]\n",
    "    def to_counter(self,stem=False):\n",
    "        self.token_content=self.content.apply(word_tokenize)\n",
    "        if stem:        \n",
    "            self.tokens=self.token_content.apply(self.stem_and_other_stuff)\n",
    "        else:\n",
    "            self.tokens=self.token_content.apply(self.check_alpha_tolower)\n",
    "        content_counter = Counter()\n",
    "        for news in self.tokens:\n",
    "            content_counter.update(news)\n",
    "        self.counter = content_counter\n",
    "\n",
    "\n",
    "class MyStopWord:\n",
    "    def __init__(self,content_counter,most_common=100,stop_word=None):\n",
    "        from nltk.corpus import stopwords\n",
    "        self.counter_stop_word=[word for word,time in content_counter.most_common(most_common)]\n",
    "        self.user_keep=[]\n",
    "        self.user_define=[]\n",
    "        if stop_word:\n",
    "            self.stop_word=stop_word\n",
    "        else:\n",
    "            self.stop_word=set(self.counter_stop_word+stopwords.words('english')) \n",
    "    def keep(self,word):\n",
    "        self.user_keep.append(word)\n",
    "        self.stop_word.discard(word)\n",
    "    def define(self,word):\n",
    "        self.user_define.append(word)\n",
    "        self.stop_word.add(word)\n",
    "\n",
    "class Unigram:\n",
    "    def __init__(self,target_counter,other_counter):\n",
    "        self.target_counter = target_counter\n",
    "        self.other_counter = other_counter\n",
    "        \n",
    "    def get_different_corpus_set(self,mystopword,TF_OTHER_THRESHOLD=20,TF_TARGET_THRESHOLD=5):\n",
    "        other_corpus_set=set(key for key,times in self.other_counter.items() if times>TF_OTHER_THRESHOLD)-mystopword.stop_word\n",
    "        target_corpus_set=set(key for key,times in self.target_counter.items() if times>TF_TARGET_THRESHOLD)-mystopword.stop_word\n",
    "        self.different_corpus_set = target_corpus_set-other_corpus_set\n",
    "\n",
    "class Bigram:\n",
    "    def __init__(self,token):\n",
    "        self.token = token\n",
    "    def count_word_pair_with_windows(self,window_size,mystopword):\n",
    "        stop_word = mystopword.stop_word\n",
    "        self.pair_counts = Counter()\n",
    "        self.pair_distance_counts = Counter()\n",
    "        for tokens in self.token:\n",
    "            for i in range(len(tokens) - 1):\n",
    "                for distance in range(1, window_size):\n",
    "                    if i + distance < len(tokens):\n",
    "                        w1 = tokens[i]\n",
    "                        w2 = tokens[i + distance]\n",
    "                        if w1 not in stop_word and w2 not in stop_word:\n",
    "                            self.pair_distance_counts[(w1, w2, distance)] += 1\n",
    "                            self.pair_counts[(w1, w2)] += 1\n",
    "\n",
    "class BigramResult:\n",
    "    def __init__(self,effective_news_date,tokens,window_size,target_bigram=False,other_bigram=False,pairwise_dictionary=\"\"):\n",
    "        if len(pairwise_dictionary)==0:\n",
    "            self.pairwise_dictionary = set(target_bigram.pair_counts.most_common(100)) - set(other_bigram.pair_counts)\n",
    "            self.pairwise_dictionary = [pair for pair,count in self.pairwise_dictionary]\n",
    "        else:\n",
    "            self.pairwise_dictionary = pairwise_dictionary\n",
    "        self.window_size=window_size\n",
    "        self.effective_news_date = effective_news_date\n",
    "        self.tokens = tokens\n",
    "    def dictionary_to_csv(self,filename):\n",
    "        oilprice_pairwise_df = pd.DataFrame({\"pairwise\": self.pairwise_dictionary})\n",
    "        oilprice_pairwise_df['pairwise1']=[x for x,y in oilprice_pairwise_df.pairwise]\n",
    "        oilprice_pairwise_df['pairwise2']=[y for x,y in oilprice_pairwise_df.pairwise]\n",
    "        oilprice_pairwise_df.to_csv(filename,sep=\";\",index = False)\n",
    "    def word_to_vector(self,mystopword):\n",
    "        self.this_year_vs=[]\n",
    "        for tokenized_article in self.tokens:\n",
    "            finder = nltk.BigramCollocationFinder.from_words([word for word in tokenized_article if word not in mystopword.stop_word],window_size=self.window_size)\n",
    "            this_vs= {key: 0 for key in pairwise_with_windows_list}\n",
    "            for pair,times in finder.ngram_fd.items():\n",
    "                if pair in this_vs.keys():\n",
    "                    this_vs[pair]=times\n",
    "            self.this_year_vs.append(this_vs)\n",
    "        self.this_year_vs_df= pd.DataFrame(self.this_year_vs)\n",
    "        self.this_year_vs_df= self.this_year_vs_df.set_index(pd.DatetimeIndex(self.tokens.index))            \n",
    "    def get_difference(self):\n",
    "        print(\"target:\",self.this_year_vs_df.loc[self.effective_news_date.values].sum(axis=1).mean())\n",
    "        print(\"other:\",self.this_year_vs_df.loc[~self.tokens.index.isin(self.effective_news_date.values)].sum(axis=1).mean())\n",
    "    def to_train_csv(self,train_filename):\n",
    "        self.this_year_vs_df['tags'] = 0\n",
    "        self.this_year_vs_df.loc[self.this_year_vs_df.index.isin(self.effective_news_date.values),'tags']=1\n",
    "        self.this_year_vs_df.to_csv(train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"../../data/crawler_news_data/oilprice_news.csv\"\n",
    "this_year_preprocessor = Preprocessor(path)\n",
    "\n",
    "effective_news_df=pd.read_csv(\"../../data/crude_oil_price/effective_news_date.csv\")\n",
    "effective_news_date = effective_news_df['date']\n",
    "effective_news_date=pd.DatetimeIndex(effective_news_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To_Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from_date = \"2017-11\"\n",
    "to_date = \"2018-09\"\n",
    "this_year_preprocessor.get_content_from_date(from_date,to_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get token and target token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eiahb\\AppData\\Local\\conda\\conda\\envs\\env_futures\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "target_content = this_year_preprocessor.content.loc[effective_news_date.values].dropna(how=\"any\")\n",
    "other_content= this_year_preprocessor.content.loc[~this_year_preprocessor.content.index.isin(effective_news_date.values)].dropna(how=\"any\")\n",
    "target_preprocessor = Preprocessor(content=target_content)\n",
    "other_preprocessor = Preprocessor(content=other_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_preprocessor.to_counter()\n",
    "other_preprocessor.to_counter()\n",
    "this_year_preprocessor.to_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mystopword=MyStopWord(content_counter=this_year_preprocessor.counter,most_common=87)\n",
    "mystopword.define('c')\n",
    "mystopword.keep('demand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get corpus set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TF_OTHER_THRESHOLD=20\n",
    "TF_TARGET_THRESHOLD=5\n",
    "\n",
    "unigram = Unigram(target_preprocessor.counter,other_preprocessor.counter)\n",
    "unigram.get_different_corpus_set(mystopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pairwise experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "window_size = 5\n",
    "target_bigram = Bigram(target_preprocessor.tokens)\n",
    "other_bigram = Bigram(other_preprocessor.tokens)\n",
    "target_bigram.count_word_pair_with_windows(window_size,mystopword)\n",
    "other_bigram.count_word_pair_with_windows(window_size,mystopword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word to Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_result = BigramResult(effective_news_date,this_year_preprocessor.tokens,window_size,target_bigram,other_bigram)\n",
    "bigram_result.word_to_vector(mystopword)\n",
    "\n",
    "filename = \"../../data/wordpair_result/oilprice_pairwise_df_window_\"+str(window_size)+\".csv\"\n",
    "bigram_result.dictionary_to_csv(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Effective date as test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def content_to_pairwise_vector(test_content,window_size,pairwise_with_windows_list):\n",
    "    test_token,test_counter = to_counter(test_content,False)\n",
    "    test_pairwise_with_distance,test_pairwise = count_word_pair_with_windows(test_token,window_size)\n",
    "    test_vs = word_to_vector(test_token,pairwise_with_windows_list,mystopword)\n",
    "    test_vs_df = pd.DataFrame(test_vs,index = test_content.index)\n",
    "    return(test_vs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_filename = \"../../data/train_test_dataset/this_year_oilprice_window_\"+str(window_size)+\"_train.csv\"\n",
    "bigram_result.to_train_csv(train_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_filename = \"../../data/train_test_dataset/this_month_oilprice_window_\"+str(window_size)+\"_test.csv\"\n",
    "effective_date = pd.read_csv(\"../../data/crude_oil_price/effective_news_date_from_2013.csv\")\n",
    "test_target_date = effective_date.loc[effective_date.date>\"2018-10-01\",\"date\"]\n",
    "path = \"../../data/crawler_news_data/oilprice_news.csv\"\n",
    "year_preprocessor = Preprocessor(path)\n",
    "year_preprocessor.get_content_from_date(\"2018-10\",\"2018-11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_preprocessor = Preprocessor(content = year_preprocessor.content)\n",
    "test_preprocessor.to_counter()\n",
    "test_bigram_result = BigramResult(effective_news_date=test_target_date,tokens=test_preprocessor.tokens,window_size=window_size,pairwise_dictionary=bigram_result.pairwise_dictionary)\n",
    "test_bigram_result.word_to_vector(mystopword=mystopword)\n",
    "test_bigram_result.to_train_csv(test_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
